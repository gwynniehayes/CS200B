---
title: "modeling"
format: html
editor_options: 
  chunk_output_type: console
---
```{r}
library(reticulate)
py_install("pandas")
py_install("pip")
py_install("plotnine")
py_install("seaborn")
py_install("scikit-learn")
py_install("XGBoost")
```

```{python}
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

seed = 42
np.random.seed(seed)
```

```{python}
y = yarn['fiber_type_id']

drop_cols = ['fiber_type_name', 'name', 'permalink', 'yarn_company_name', 'fiber_type_id']
X = yarn.drop(columns=drop_cols)

categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

print("Features shape:", X.shape)
```

```{python}
numeric_features = X.select_dtypes(include=[np.number]).columns
categorical_features = X.select_dtypes(exclude=[np.number]).columns

imputer = SimpleImputer(strategy='median')

X[numeric_features] = imputer.fit_transform(X[numeric_features])
```

```{python}
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=seed, stratify=y)
```

```{python}
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()

numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])
preprocessor = ColumnTransformer([('num', numeric_transformer, numeric_features)], remainder='passthrough')

dt = DecisionTreeClassifier(random_state=seed)
knn = KNeighborsClassifier()
log_reg = LogisticRegression(
    max_iter=9000,
    solver='saga',
    multi_class='multinomial',
    random_state=seed)
rf = RandomForestClassifier(random_state=seed)
rf = RandomForestClassifier(random_state=seed)
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=seed)

pipe = Pipeline([
    ('preprocessor', preprocessor),
    ('pca', 'passthrough'),
    ('classifier', 'passthrough')
])
```

```{python}
param_grid = [
    # Decision Tree
    {
        'classifier': [dt],
        'classifier__criterion': ['gini', 'entropy'],
        'classifier__max_depth': [None, 5, 10],
        'classifier__min_samples_split': [2, 5],
        'classifier__min_samples_leaf': [1, 2]
    },
    
    # kNN
    {
        'pca': ['passthrough', PCA(n_components=0.95)],
        'classifier': [knn],
        'classifier__n_neighbors': [5, 7, 9],
        'classifier__weights': ['uniform', 'distance']
    },
    
    # Logistic Regression
    {
        'pca': ['passthrough', PCA(n_components=0.95)],
        'classifier': [log_reg],
        'classifier__C': [0.01, 0.1, 1, 5, 10]
    },
    
    # Random Forest
    {
        'classifier': [rf],
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [None, 5, 10],
        'classifier__min_samples_split': [2, 5],
        'classifier__min_samples_leaf': [1, 2]
    },
    
    # XGBoost
    {
        'classifier': [xgb],
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [3, 5, 7],
        'classifier__learning_rate': [0.01, 0.1, 0.2],
        'classifier__subsample': [0.7, 1.0]
    }]
```

```{python}
grid_search = GridSearchCV(
    estimator=pipe,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    scoring='accuracy',
    verbose=2
)

grid_search.fit(X_train, y_train)
```

```{python}
best_model = grid_search.best_estimator_
print("Best Parameters:\n", grid_search.best_params_)

# Predictions
y_pred = best_model.predict(X_test)

# Accuracy
print("Test Accuracy:", accuracy_score(y_test, y_pred))

# Classification report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=best_model.named_steps['classifier'].classes_,
            yticklabels=best_model.named_steps['classifier'].classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()
```

